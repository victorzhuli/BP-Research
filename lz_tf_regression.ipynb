{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create bottlenecks\n",
    "%run lz_producebottlenecks.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile lz_classifier.py\n",
    "'''\n",
    "revised the method for split training and testing dataset\n",
    "\n",
    "'''\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import argparse\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.platform import gfile\n",
    "from progress.bar import Bar\n",
    "\n",
    "def lz_compute_bottleneck(bottleneck_dir):\n",
    "    '''\n",
    "    Given \"bottleneck_dir\", load computed bottleneck data and labels\n",
    "    Output: the bottleneck data and labels\n",
    "    '''\n",
    "    ### LOAD DATA FROM BOTTLENECKS\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    test_inputs = []\n",
    "    test_labels = []\n",
    "\n",
    "    bottleneck_list = []\n",
    "    file_glob = os.path.join(bottleneck_dir, '*.txt')\n",
    "    bottleneck_list.extend(gfile.Glob(file_glob))\n",
    "    # shuffle(bottleneck_list, random_state = 1)\n",
    "    \n",
    "    last_win_train = lz_locate_last_train_window(bottleneck_dir)\n",
    "\n",
    "    for bottleneck_file in bottleneck_list:\n",
    "\n",
    "        bottleneck = open(bottleneck_file)\n",
    "        bottleneck_string = bottleneck.read()\n",
    "        bottleneck_values = [float(x) for x in bottleneck_string.split(',')]\n",
    "        \n",
    "        regex = re.compile(r'\\d+')\n",
    "        labels = regex.findall(bottleneck_file)\n",
    "        SBP_label = int(labels[-2])\n",
    "        DBP_label = int(labels[-1])\n",
    "        win_index = int(labels[-3])  #extract window index for this bottleneck file\n",
    "        \n",
    "        if win_index > last_win_train:\n",
    "            train_inputs.append(bottleneck_values)\n",
    "            train_labels.append([int(SBP_label), int(DBP_label)])\n",
    "        else:\n",
    "            test_inputs.append(bottleneck_values)\n",
    "            test_labels.append([int(SBP_label), int(DBP_label)])\n",
    "            \n",
    "    val_inputs = test_inputs; val_labels = test_labels\n",
    "        \n",
    "    return train_inputs, val_inputs, test_inputs, train_labels, val_labels, test_labels\n",
    "\n",
    "def lz_locate_last_train_window(bottleneck_dir):\n",
    "    '''\n",
    "    Given window length, assign data into training, validate and testing sets\n",
    "    In this analysis, validate data equivalent to testing data\n",
    "    For each trial, window index belongs to first 10 second is assigned to training data\n",
    "    The last 2 second is assinged to testing data.\n",
    "    Output: the window index which last to the end of the first 10 second of the trial\n",
    "    '''\n",
    "    \n",
    "    '''Extract the window length'''\n",
    "    win_len = int(bottleneck_dir[-14])\n",
    "    new_srate = 100\n",
    "    win_step = 0.1 * new_srate\n",
    "    win_len = int(bottleneck_dir[-14]) * new_srate\n",
    "    \n",
    "    '''Find the window index which with end at 10 second'''\n",
    "    last_win_train = math.ceil((10*new_srate - win_len)/win_step + 1)\n",
    "    first_win_test = \n",
    "        \n",
    "    return last_win_train\n",
    "\n",
    "def main(learning_rate = 0.0005, batch_size = 64, epochs = 2500, log_batch_step = 50):\n",
    "    train_inputs, valtest_inputs, test_inputs, train_labels, valtest_labels, test_labels = lz_compute_bottleneck(bottleneck_dir)\n",
    "    \n",
    "    # useful info\n",
    "    n_features = np.size(train_inputs, 1)\n",
    "    n_labels = np.size(train_labels, 1)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    # Placeholders for input features and labels\n",
    "    inputs = tf.placeholder(tf.float32, (None, n_features), name='inputs')\n",
    "    labels = tf.placeholder(tf.float32, (None, n_labels), name='labels')\n",
    "    label_SBP, label_DBP = tf.split(labels, 2, 1)\n",
    "\n",
    "    # Setting up weights and bias\n",
    "    weights = tf.Variable(tf.truncated_normal((n_features, n_labels), stddev=0.1), name='weights')\n",
    "    bias = tf.Variable(tf.zeros(n_labels), name='bias')\n",
    "    tf.summary.histogram('weightshist', weights)\n",
    "    tf.summary.histogram('biashist', bias)\n",
    "    \n",
    "    # Setting up operation in fully connected layer\n",
    "    predictions = tf.add(tf.matmul(inputs, weights), bias)\n",
    "    pred_SBP, pred_DBP = tf.split(predictions, 2, 1)\n",
    "\n",
    "    # Defining loss of network\n",
    "    mean_squared_error_SBP = tf.losses.mean_squared_error(pred_SBP, label_SBP)\n",
    "    mean_squared_error_DBP = tf.losses.mean_squared_error(pred_DBP, label_DBP)\n",
    "    total_loss = mean_squared_error_SBP + mean_squared_error_DBP\n",
    "\n",
    "    tf.summary.scalar('loss', total_loss)\n",
    "    \n",
    "    # Setting optimiser\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "    # For saving checkpoint after training\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # use in command line: tensorboard --logdir=path/to/log  --> to view tensorboard\n",
    "\n",
    "    # Run tensorflow session\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        train_writer = tf.summary.FileWriter('log', sess.graph)\n",
    "        tf.train.write_graph(sess.graph_def, '', 'savedgraph.pbtxt', as_text=False)\n",
    "\n",
    "        # Running the training in batches \n",
    "        batch_count = int(math.ceil(len(train_inputs)/batch_size))\n",
    "\n",
    "        for epoch_i in range(epochs):\n",
    "            batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{}'.format(epoch_i+1, epochs), unit='batches')\n",
    "            # The training cycle\n",
    "            for batch_i in batches_pbar:\n",
    "                # Get a batch of training features and labels\n",
    "                batch_start = batch_i*batch_size\n",
    "                batch_inputs = train_inputs[batch_start:batch_start + batch_size]\n",
    "                batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
    "                # Run optimizer\n",
    "                _, summary = sess.run([optimizer, merged], feed_dict={inputs: batch_inputs, labels: batch_labels})\n",
    "                train_writer.add_summary(summary, batch_i)\n",
    "\n",
    "            # Check mean sqaure error against validation data\n",
    "            val_MSR_SBP,val_MSR_DBP,val_total_loss = sess.run([mean_squared_error_SBP,mean_squared_error_DBP,total_loss], feed_dict={inputs: val_inputs, labels: val_labels})\n",
    "            print(\"After epoch {}, MSR_SBP: {}, MSR_DBP: {}, Total Loss: {}.\".format(epoch_i+1, val_MSR_SBP,val_MSR_DBP,val_total_loss))\n",
    "\n",
    "        test_MSR_SBP,test_MSR_DBP,test_total_loss = sess.run([mean_squared_error_SBP,mean_squared_error_DBP,total_loss], feed_dict={inputs: test_inputs, labels: test_labels})\n",
    "        print (\"Test MSR_SBP: {}, Test MSR_DBP: {}, Test Total Loss: {}.\".format(test_MSR_SBP,test_MSR_DBP,test_total_loss))\n",
    "\n",
    "        test_predictions = sess.run([predictions], feed_dict={inputs: test_inputs, labels: test_labels})\n",
    "\n",
    "        g = tf.get_default_graph()\n",
    "        saver.save(sess, 'savedgraph')\n",
    "        \n",
    "        return test_predictions\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # %run lz_producebottlenecks.py\n",
    "    slim = tf.contrib.slim\n",
    "    bottleneck_dir = 'VG_PPG_file2_7s_bottlenecks'\n",
    "    # Setting hyperparameters\n",
    "    learning_rate = 0.0005\n",
    "    batch_size = 64\n",
    "    epochs = 2500\n",
    "    log_batch_step = 50\n",
    "\n",
    "    main(learning_rate, batch_size, epochs, log_batch_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# from scipy.stats import ttest_ind\n",
    "\n",
    "pred = test_predictions[0]\n",
    "label = (np.array(test_labels))\n",
    "\n",
    "DBP_pred = pred[:,1]\n",
    "DBP_label = label[:,1]\n",
    "DBP_R = np.corrcoef(DBP_pred, DBP_label)\n",
    "DBP_MAE = (np.absolute((DBP_pred - DBP_label))).mean()\n",
    "DBP_MAE_std = np.std((DBP_pred - DBP_label))\n",
    "DBP_RMSE = np.sqrt(((DBP_pred - DBP_label) ** 2).mean()) \n",
    "# t_DBP, p_DBP = ttest_ind(DBP_pred, DBP_label)\n",
    "print(\"R of DBP    = {}\".format(DBP_R[0][1]))\n",
    "# print(\"p of DBP    = {}\".format(p_DBP))\n",
    "print(\"MAE of DBP  = {} +/- {}\".format(DBP_MAE, DBP_MAE_std))\n",
    "print(\"RMSE of DBP = {}\\n\".format(DBP_RMSE))\n",
    "plt.plot(DBP_label, DBP_pred, \"o\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([60,120])\n",
    "axes.set_ylim([60,120])\n",
    "plt.show()\n",
    "\n",
    "SBP_pred = pred[:,0]\n",
    "SBP_label = label[:,0]\n",
    "SBP_R = np.corrcoef(SBP_pred, SBP_label)\n",
    "SBP_MAE = (np.absolute((SBP_pred - SBP_label))).mean()\n",
    "SBP_MAE_std = np.std((SBP_pred - SBP_label))\n",
    "SBP_RMSE = np.sqrt(((SBP_pred - SBP_label) ** 2).mean())\n",
    "print(\"R of SBP    = {}\".format(SBP_R[0][1]))\n",
    "print(\"MAE of DBP  = {} +/- {}\".format(SBP_MAE, SBP_MAE_std))\n",
    "print(\"RMSE of SBP = {}\".format(SBP_RMSE))\n",
    "plt.plot(SBP_label, SBP_pred, \"o\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([90,170])\n",
    "axes.set_ylim([90,170])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plot of the results across DBP and SBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_pred  = np.concatenate((DBP_pred, SBP_pred))\n",
    "comb_label = np.concatenate((DBP_label, SBP_label))\n",
    "plt.plot(comb_label, comb_pred, \"bo\", markersize=1)\n",
    "plt.plot(comb_label, comb_label, \"ro\", markersize=4)\n",
    "plt.xlabel('Blood Cuff Measure (mmHg)', fontsize = '16')\n",
    "plt.ylabel('PPG Based Estimation (mmHg)', fontsize = '16')\n",
    "plt.savefig(\"BP_fitting_7s.svg\", format=\"svg\", bbox_inches='tight')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([60,170])\n",
    "axes.set_ylim([60,170])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bland-Altman analysis across DBP and SBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplot(131)\n",
    "# plt.plot(comb_label, diff_pred_label, \"bo\", markersize=1)\n",
    "# axes = plt.gca()\n",
    "# axes.set_xlim([60,170])\n",
    "# axes.set_ylim([-60,60])\n",
    "# plt.title(\"\")\n",
    "\n",
    "\n",
    "diff_pred_label_DBP = DBP_pred - DBP_label\n",
    "diff_pred_label_SBP = SBP_pred - SBP_label\n",
    "plt.subplot(131)\n",
    "plt.hist(diff_pred_label_DBP, 50, normed=1, histtype='stepfilled')  # arguments are passed to np.histogram\n",
    "plt.title(\"DBP Error\")\n",
    "plt.subplot(132)\n",
    "plt.hist(diff_pred_label_SBP, 50, normed=1, histtype='stepfilled')  # arguments are passed to np.histogram\n",
    "plt.title(\"SBP Error\")\n",
    "plt.savefig(\"BP_fitting_7s_error.svg\", format=\"svg\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_DBP = diff_pred_label_DBP.size\n",
    "small_than_05_DBP = sum(i < 5 for i in diff_pred_label_DBP)\n",
    "small_than_10_DBP = sum(i < 10 for i in diff_pred_label_DBP)\n",
    "small_than_15_DBP = sum(i < 15 for i in diff_pred_label_DBP)\n",
    "percentage_small_than_05_DBP = small_than_05_DBP / total_DBP\n",
    "percentage_small_than_10_DBP = small_than_10_DBP / total_DBP\n",
    "percentage_small_than_15_DBP = small_than_15_DBP / total_DBP\n",
    "\n",
    "total_SBP = diff_pred_label_SBP.size\n",
    "small_than_05_SBP = sum(i < 5 for i in diff_pred_label_SBP)\n",
    "small_than_10_SBP = sum(i < 10 for i in diff_pred_label_SBP)\n",
    "small_than_15_SBP = sum(i < 15 for i in diff_pred_label_SBP)\n",
    "percentage_small_than_05_SBP = small_than_05_SBP / total_SBP\n",
    "percentage_small_than_10_SBP = small_than_10_SBP / total_SBP\n",
    "percentage_small_than_15_SBP = small_than_15_SBP / total_SBP\n",
    "\n",
    "print (\"DBP < 5 = {:.2f}%, < 10 = {:.2f}%, < 15 = {:.2f}%.\\n\".format(percentage_small_than_05_DBP*100, percentage_small_than_10_DBP*100, percentage_small_than_15_DBP*100))\n",
    "print (\"SBP < 5 = {:.2f}%, < 10 = {:.2f}%, < 15 = {:.2f}%.\\n\".format(percentage_small_than_05_SBP*100, percentage_small_than_10_SBP*100, percentage_small_than_15_SBP*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lz_BlandAltman_correlation as BA\n",
    "diff_pred_label = comb_pred - comb_label\n",
    "# plt.subplot(131)\n",
    "# plt.plot(comb_label, diff_pred_label, \"bo\", markersize=1)\n",
    "# axes = plt.gca()\n",
    "# axes.set_xlim([60,170])\n",
    "# axes.set_ylim([-60,60])\n",
    "# plt.title(\"\")\n",
    "\n",
    "# plt.subplot(132)\n",
    "# plt.hist(diff_pred_label, bins='auto')  # arguments are passed to np.histogram\n",
    "# plt.title(\"Histogram of Difference\")\n",
    "\n",
    "# plt.subplot(133)\n",
    "BA.lz_BlandAltmanPlot(comb_pred, comb_label)\n",
    "plt.title(\"Bland-Altman\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([60,170])\n",
    "\n",
    "font = {'family': 'serif',\n",
    "        'color':  'darkred',\n",
    "        'weight': 'normal',\n",
    "        'size': 24,\n",
    "        }\n",
    "\n",
    "Difference = comb_pred-comb_label\n",
    "mean_diff = np.mean(Difference)\n",
    "Std_diff = np.std(Difference)\n",
    "upper_limit = mean_diff + 1.96*Std_diff\n",
    "lower_limit = mean_diff - 1.96*Std_diff\n",
    "plt.text(165, mean_diff, 'Mean Diff = %0.6s' % mean_diff, fontdict=font)\n",
    "plt.text(165, upper_limit, '+1.96SD = %0.6s' % upper_limit, fontdict=font)\n",
    "plt.text(165, lower_limit, '-1.96SD = %0.6s' % lower_limit, fontdict=font)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run lz_BlandAltman_correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE for individual BP points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "mean_err_uniq_est = []\n",
    "std_err_uniq_est  = []\n",
    "rmse_uniq_est     = []\n",
    "uniq_label_list = np.unique(comb_label)\n",
    "for uniq_label in uniq_label_list:\n",
    "    mean_err_uniq_est.append(np.mean(Difference[comb_label == uniq_label]))\n",
    "    std_err_uniq_est.append(np.std(Difference[comb_label == uniq_label]))\n",
    "    rmse_uniq_est.append(sqrt(mean_squared_error(comb_pred[comb_label == uniq_label], comb_label[comb_label == uniq_label])))\n",
    "\n",
    "plt.plot(uniq_label_list, rmse_uniq_est, 'o')\n",
    "plt.xlabel('Blood Cuff Measure (mmHg)', fontsize = '16')\n",
    "plt.ylabel('RMSE(mmHg)', fontsize = '16')\n",
    "plt.show()\n",
    "np.mean(rmse_uniq_est)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
